<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Hossein Taheri — Academic Website</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
body { font-family: Arial, sans-serif; max-width: 900px; margin:auto; padding:20px; line-height:1.6; }
h1, h2 { color:#333; }
a { color:#0645AD; text-decoration:none; }
a:hover { text-decoration:underline; }
.section { margin-top:40px; }
</style>
</head>
<body>
<h1><strong>Hossein Taheri</strong></h1>
<p>
Postdoctoral Scholar, Computer Science & Engineering<br>
University of California, San Diego<br>
Email: htaheri[at]ucsd[dot]edu<br>
<a href="https://scholar.google.com/citations?user=J_MeH1gAAAAJ&hl=en">Google Scholar</a> 
</p>
<div class="section">
<h2>Bio</h2>
<p>
I am a Postdoctoral Scholar in the Department of Computer Science and Engineering at UC San Diego, 
advised by <a href="https://mazumdar.ucsd.edu/" target="_blank">Prof. Arya Mazumdar</a>. 
Previously, I obtained my Ph.D. in Electrical and Computer Engineering from UC Santa Barbara, 
where I was advised by 
<a href="https://sites.google.com/view/cthrampo" target="_blank">Prof. Christos Thrampoulidis</a>. 
I completed my B.Sc. in Electrical Engineering and Mathematics at Sharif University of Technology.
</p>
</div>
<div class="section">
<h2>Research Interests </h2>
<ul>
<li>Deep Learning Theory, Generalization and Optimization of Neural Networks</li>
<li>Transformers and in-context learning</li>
<li>Continual Learning</li>
<li>Distributed Optimization</li>
<li>High-dimensional Statistics</li>
</ul>
</div>
<div class="section">
<h2>Publications</h2>
<ol>
<li>
<strong><a href="https://arxiv.org/pdf/2510.05573">[link]</a> On the Theory of Continual Learning with Gradient Descent for Neural Networks</strong><br>
H. Taheri, A. Ghosh, A. Mazumdar (Preprint).
</li>
<li>
<strong><a href="https://openreview.net/forum?id=h7GAgbLSmC">[link]</a> Sharper Guarantees for Learning Neural Network Classifiers with Gradient Methods</strong><br>
H. Taheri, C. Thrampoulidis, A. Mazumdar (ICLR 2025).
</li>
<li>
<strong><a href="https://www.jmlr.org/papers/v25/23-0422.html">[link]</a> Generalization and Stability of Interpolating Neural Networks with Minimal Width</strong><br>
H. Taheri, C. Thrampoulidis (JMLR 2024).
</li>
<li>
<strong><a href="https://openreview.net/pdf?id=wTGjn7JvYK">[link]</a> On the Optimization and Generalization of Multi-head Attention</strong><br>
P. Deora*, R. Ghaderi*, H. Taheri*, C. Thrampoulidis (TMLR 2024, selected for ICLR 2025 poster).
</li>
<li>
<strong><a href="https://proceedings.mlr.press/v206/taheri23a.html">[link]</a> On Generalization of Decentralized Learning with Separable Data</strong><br>
H. Taheri, C. Thrampoulidis (AISTATS 2023).
</li>
<li>
<strong><a href="https://arxiv.org/pdf/2305.13471">[link]</a> Fast Convergence in Learning Two-layer Neural Networks with Separable Data</strong><br>
H. Taheri, C. Thrampoulidis (AAAI 2023).
</li>
<li>
<strong><a href="https://arxiv.org/abs/2010.13275">[link]</a> Asymptotic Behavior of Adversarial Training in Binary Linear Classification</strong><br>
H. Taheri, R. Pedarsani, C. Thrampoulidis (IEEE TNNLS 2023).
</li>
<li>
<strong><a href="https://proceedings.mlr.press/v130/taheri21a.html">[link]</a> Fundamental Limits of Ridge-Regularized Empirical Risk Minimization in High Dimensions</strong><br>
H. Taheri, R. Pedarsani, C. Thrampoulidis (AISTATS 2021).
</li>
<li>
<strong><a href="https://proceedings.mlr.press/v119/taheri20a.html">[link]</a> Quantized Decentralized Stochastic Learning over Directed Graphs</strong><br>
H. Taheri, A. Mokhtari, H. Hassani, R. Pedarsani (ICML 2020).
</li>
<li>
<strong><a href="https://proceedings.mlr.press/v108/taheri20a.html">[link]</a> Sharp Asymptotics and Optimal Performance for Inference in Binary Models</strong><br>
H. Taheri, R. Pedarsani, C. Thrampoulidis (AISTATS 2020).
</li>
<li>
<strong><a href="https://arxiv.org/pdf/1907.10595">[link]</a> Robust and Communication-efficient Collaborative Learning</strong><br>
A. Reisizadeh, H. Taheri, A. Mokhtari, H. Hassani, R. Pedarsani (NeurIPS 2019).
</li>
</ol>
</div>
</body>
</html>
